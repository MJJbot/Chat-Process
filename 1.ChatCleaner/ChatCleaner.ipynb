{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import codecs\n",
    "import re\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "length_minimum = 5\n",
    "set_minimum = 5\n",
    "whitelist_regex = re.compile(r\"([^!@#$%^&*(),.?\\\":{}|`~<>\\[\\]\\-_=+\\\\/;'a-zA-Z0-9\\u3131-\\u318E\\uAC00-\\uD7A3\\u1100-\\u11f9\\s]|http)\")\n",
    "normalize_regexs = [(re.compile(r\"([!@#$%^&*(),.?\\\":{}|`~<>\\[\\]\\-_=+\\\\/;'\\u3131-\\u318E\\uAC00-\\uD7A3])(\\1{2})\\1+\"), \"\\\\1\\\\2\"),\n",
    "                    (re.compile(r\"[\\u314C\\u314B]{2,}\"), \"\\u314B\"), # zㅌㅋ -> \"ㅋ\"\n",
    "                    (re.compile(r\"[\\u3154\\u3156]{2,}\"), \"\\u314B\"), # ㅔㅖ -> \"ㅔ\"\n",
    "                    (re.compile(r\"[\\u3160\\u315C]{2,}\"), \"\\u3160\"), # ㅠㅜ -> \"ㅠ\"\n",
    "                    (re.compile(r\"(!|\\?)\\1+\"), \"\\\\1\"), # ?, ! -> \"?\", \"!\"\n",
    "                    (re.compile(r\"(\\.|\\,|\\;)\\1+\"), \"\\\\1\\\\1\")] # '.', ',', ';' -> \"..\", \",,\", \";;\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_chats(filename):\n",
    "    with open(filename, 'r', encoding='utf-8-sig') as data_file:\n",
    "        return data_file.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_system_message(sentence):\n",
    "    if \"subscribe\" in sentence:\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_simple_sentence(sentence):\n",
    "    characters = set()\n",
    "    for char in sentence:\n",
    "        if char not in characters:\n",
    "            characters.add(char)\n",
    "            if len(characters) >= set_minimum:\n",
    "                return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_question(sentence):\n",
    "    if \"?\" not in sentence:\n",
    "        return False\n",
    "    if sentence[0] == \"?\" or sentence[1] == \"?\":\n",
    "        return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_contain_other_character(sentence):\n",
    "    if whitelist_regex.search(sentence) is None:\n",
    "        return False\n",
    "    else:\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_duplicated(sentence, sentence_set):\n",
    "    if sentence in sentence_set:\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_sentence(sentence):\n",
    "    processing_time = [0] * len(normalize_regexs)\n",
    "    normalized_count = [0] * len(normalize_regexs)\n",
    "    for idx, regex in enumerate(normalize_regexs):\n",
    "        routine_start_time = time.time()\n",
    "        sentence, normalized_count[idx] = regex[0].subn(regex[1], sentence)\n",
    "        processing_time[idx] = time.time() - routine_start_time\n",
    "    return sentence, processing_time, [1 if sum(normalized_count) > 0 else 0, normalized_count]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_chats(chats):\n",
    "    start_time = time.time()\n",
    "    processing_time = {length: 0, system: 0, simple: 0, filter: 0, normalize: [0] * len(normalize_regexs), duplicate: 0, question: 0]\n",
    "\n",
    "    chat_set = set()\n",
    "    chat_list = list()\n",
    "    question_list = list()\n",
    "\n",
    "    skipped_chat_count = {filter: 0, length: 0, system: 0, simple: 0, duplicate: 0}  # filter, length, system, simple, duplicate\n",
    "    processed_chat_count = [chat: 0, question: 0  # chat, question\n",
    "    normalized_count = [0, [0] * len(normalize_regexs)]\n",
    "\n",
    "    for chat in chats:\n",
    "        if chat[-1] == \"\\n\":\n",
    "            chat = chat[:-1]\n",
    "\n",
    "        # 1. check length\n",
    "        routine_start_time = time.time()\n",
    "        if len(chat) < length_minimum:\n",
    "            skipped_chat_count[1] += 1\n",
    "            continue\n",
    "        processing_time[0] += time.time() - routine_start_time\n",
    "\n",
    "        # 2. check system message\n",
    "        routine_start_time = time.time()\n",
    "        if is_system_message(chat):\n",
    "            skipped_chat_count[2] += 1\n",
    "            continue\n",
    "        processing_time[1] += time.time() - routine_start_time\n",
    "\n",
    "        # 3. check simple sentence\n",
    "        routine_start_time = time.time()\n",
    "        if is_simple_sentence(chat):\n",
    "            skipped_chat_count[3] += 1\n",
    "            continue\n",
    "        processing_time[2] += time.time() - routine_start_time\n",
    "\n",
    "        # 0. filter characters\n",
    "        routine_start_time = time.time()\n",
    "        if is_contain_other_character(chat):\n",
    "            skipped_chat_count[0] += 1\n",
    "            continue\n",
    "        processing_time[3] += time.time() - routine_start_time\n",
    "        \n",
    "        # Normalize sentence\n",
    "        chat, normalize_time, count = normalize_sentence(chat)\n",
    "        normalized_count[0] += count[0]\n",
    "        for idx, val in enumerate(normalize_time):\n",
    "            processing_time[4][idx] += val\n",
    "        for idx, val in enumerate(count[1]):\n",
    "            normalized_count[1][idx] += val\n",
    "\n",
    "        # 4. check duplication\n",
    "        routine_start_time = time.time()\n",
    "        if is_duplicated(chat, chat_set):\n",
    "            skipped_chat_count[4] += 1\n",
    "            continue\n",
    "        processing_time[5] += time.time() - routine_start_time\n",
    "\n",
    "        chat_set.add(chat)\n",
    "        chat_list.append(chat + \"\\n\")\n",
    "        processed_chat_count[0] += 1\n",
    "\n",
    "        # 5. check question\n",
    "        routine_start_time = time.time()\n",
    "        if not is_question(chat):\n",
    "            continue\n",
    "        processing_time[6] += time.time() - routine_start_time\n",
    "\n",
    "        question_list.append(chat + \"\\n\")\n",
    "        processed_chat_count[1] += 1\n",
    "\n",
    "    return chat_list, question_list, [time.time() - start_time, processing_time], [skipped_chat_count, processed_chat_count, normalized_count]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_chats_and_questions(filename, chat_list, question_list):\n",
    "    filename_split = filename.split(\".\")\n",
    "    chat_filename = \".\".join(filename_split[:-1]) + \"_chat.\" + filename_split[-1]\n",
    "    question_filename = \".\".join(filename_split[:-1]) + \"_question.\" + filename_split[-1]\n",
    "\n",
    "    with codecs.open(chat_filename, 'w', encoding='utf-8') as output_file:\n",
    "        output_file.writelines(chat_list)\n",
    "    with codecs.open(question_filename, 'w', encoding='utf-8') as output_file:\n",
    "        output_file.writelines(question_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_result(processing_time, count):\n",
    "    total_time = processing_time[0]\n",
    "    routine_time = processing_time[1]\n",
    "    \n",
    "    skipped = count[0]\n",
    "    processed = count[1]\n",
    "    normalized = count[2]\n",
    "\n",
    "    total_skipped = sum(skipped)\n",
    "    total = total_skipped + processed[0]\n",
    "    total_normalized = sum(normalized[1])\n",
    "\n",
    "    \n",
    "    print(\"\")\n",
    "    print(\"Total processing time: %.3f secs\" % (total_time))\n",
    "    print(\"0. Check length     : %.3f secs\" % (routine_time[0]))\n",
    "    print(\"1. Check system msg : %.3f secs\" % (routine_time[1]))\n",
    "    print(\"2. Check simplicity : %.3f secs\" % (routine_time[2]))\n",
    "    print(\"3. Check char filter: %.3f secs\" % (routine_time[3]))\n",
    "    print(\"4. Normalize chat   : %.3f secs\" % (sum(routine_time[4])))\n",
    "    print(\"    0. Normalize general: %.3f secs\" % (routine_time[4][0]))\n",
    "    print(\"    1. Normalize ㅋㅋ    : %.3f secs\" % (routine_time[4][1]))\n",
    "    print(\"    2. Normalize ㅔㅖ    : %.3f secs\" % (routine_time[4][2]))\n",
    "    print(\"    3. Normalize ㅠㅜ    : %.3f secs\" % (routine_time[4][3]))\n",
    "    print(\"    4. Normalize ?!     : %.3f secs\" % (routine_time[4][4]))\n",
    "    print(\"    5. Normalize .,;    : %.3f secs\" % (routine_time[4][5]))\n",
    "    print(\"5. Check duplication: %.3f secs\" % (routine_time[5]))\n",
    "    print(\"6. Check question   : %.3f secs\" % (routine_time[0]))\n",
    "    print(\"\")\n",
    "    print(\"Total chats: %d\" % total)\n",
    "    print(\"Skipped chats: %d (%.2f%%)\" % (total_skipped, total_skipped / total * 100))\n",
    "    print(\"  0. Skipped chats by char filter : %d (%.2f%%)\" % (skipped[0], skipped[0] / total_skipped * 100))\n",
    "    print(\"  1. Skipped chats by short length: %d (%.2f%%)\" % (skipped[1], skipped[1] / total_skipped * 100))\n",
    "    print(\"  2. Skipped chats by system msg  : %d (%.2f%%)\" % (skipped[2], skipped[2] / total_skipped * 100))\n",
    "    print(\"  3. Skipped chats by simplicity  : %d (%.2f%%)\" % (skipped[3], skipped[3] / total_skipped * 100))\n",
    "    print(\"  4. Skipped chats by duplication : %d (%.2f%%)\" % (skipped[4], skipped[4] / total_skipped * 100))\n",
    "    print(\"Processed chats: %d (%.2f%%)\" % (processed[0], processed[0] / total * 100))\n",
    "    print(\"  Processed normal chats: %d (%.2f%%)\" % (processed[0] - processed[1], 100 - (processed[1] / processed[0] * 100)))\n",
    "    print(\"  Processed questions   : %d (%.2f%%)\" % (processed[1], processed[1] / processed[0] * 100))\n",
    "    print(\"\")\n",
    "    print(\"Normalized chats: %d (%.2f%%), total %d words\" % (normalized[0], normalized[0] / processed[0] * 100, total_normalized))\n",
    "    print(\"  0. Normalized general: %d (%.2f%%)\" % (normalized[1][0], normalized[1][0] / total_normalized * 100))\n",
    "    print(\"  1. Normalized ㅋㅋ    : %d (%.2f%%)\" % (normalized[1][1], normalized[1][1] / total_normalized * 100))\n",
    "    print(\"  2. Normalized ㅔㅖ    : %d (%.2f%%)\" % (normalized[1][2], normalized[1][2] / total_normalized * 100))\n",
    "    print(\"  3. Normalized ㅠㅜ    : %d (%.2f%%)\" % (normalized[1][3], normalized[1][3] / total_normalized * 100))\n",
    "    print(\"  4. Normalized ?!     : %d (%.2f%%)\" % (normalized[1][4], normalized[1][4] / total_normalized * 100))\n",
    "    print(\"  5. Normalized .,;    : %d (%.2f%%)\" % (normalized[1][5], normalized[1][5] / total_normalized * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "filename:  ../../Data/1to100.txt\n",
      "\n",
      "Total processing time: 557.858 secs\n",
      "0. Check length     : 7.885 secs\n",
      "1. Check system msg : 11.159 secs\n",
      "2. Check simplicity : 45.809 secs\n",
      "3. Check char filter: 28.469 secs\n",
      "4. Normalize chat   : 249.004 secs\n",
      "  0. Normalize general: 83.430 secs\n",
      "  1. Normalize ㅋㅋ    : 25.612 secs\n",
      "  2. Normalize ㅔㅖ    : 23.766 secs\n",
      "  3. Normalize ㅠㅜ    : 23.208 secs\n",
      "  4. Normalize ?!     : 46.105 secs\n",
      "  5. Normalize .,;    : 46.883 secs\n",
      "5. Check duplication: 19.531 secs\n",
      "6. Check question   : 7.885 secs\n",
      "\n",
      "Total chats: 49537148\n",
      "Skipped chats: 30616206 (61.80%)\n",
      "  0. Skipped chats by char filter : 414105 (1.35%)\n",
      "  1. Skipped chats by short length: 15496703 (50.62%)\n",
      "  2. Skipped chats by system msg  : 41955 (0.14%)\n",
      "  3. Skipped chats by simplicity  : 9794111 (31.99%)\n",
      "  4. Skipped chats by duplication : 4869332 (15.90%)\n",
      "Processed chats: 18920942 (38.20%)\n",
      "  Processed normal chats: 16956693 (89.62%)\n",
      "  Processed questions   : 1964249 (10.38%)\n",
      "\n",
      "Normalized chats: 5254033 (27.77%), total 7814475 words\n",
      "  0. Normalized general: 2309790 (29.56%)\n",
      "  1. Normalized ㅋㅋ    : 3292316 (42.13%)\n",
      "  2. Normalized ㅔㅖ    : 1453 (0.02%)\n",
      "  3. Normalized ㅠㅜ    : 267399 (3.42%)\n",
      "  4. Normalized ?!     : 533367 (6.83%)\n",
      "  5. Normalized .,;    : 1410150 (18.05%)"
     ]
    }
   ],
   "source": [
    "filename = input(\"filename: \")\n",
    "chats = get_chats(filename)\n",
    "chat_list, question_list, processing_time, count = clean_chats(chats)\n",
    "save_chats_and_questions(filename, chat_list, question_list)\n",
    "analyze_result(processing_time, count)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
